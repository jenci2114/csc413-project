# -*- coding: utf-8 -*-
"""Copy of Copy of sd_textual_inversion_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14B2Erte7SDVSBtDzpLE_tMp1lS9kT8Cy

# Textual-inversion fine-tuning for Stable Diffusion using dðŸ§¨ffusers 

This notebook shows how to "teach" Stable Diffusion a new concept via textual-inversion using ðŸ¤— Hugging Face [ðŸ§¨ Diffusers library](https://github.com/huggingface/diffusers). 

![Textual Inversion example](https://textual-inversion.github.io/static/images/editing/colorful_teapot.JPG)
_By using just 3-5 images you can teach new concepts to Stable Diffusion and personalize the model on your own images_ 

For a general introduction to the Stable Diffusion model please refer to this [colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb).

## Initial setup
"""
import re
import argparse
import accelerate
from torch.utils.tensorboard import SummaryWriter
import argparse
import itertools
import math
import os
import random

import numpy as np
import torch
import torch.nn.functional as F
import torch.utils.checkpoint
from torch.utils.data import Dataset

import PIL
from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import set_seed
from diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel
from diffusers.optimization import get_scheduler
from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
from PIL import Image
from torchvision import transforms
from tqdm.auto import tqdm
from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer


from typing import Dict


def image_grid(imgs, rows, cols):
    assert len(imgs) == rows*cols

    w, h = imgs[0].size
    grid = Image.new('RGB', size=(cols*w, rows*h))
    grid_w, grid_h = grid.size
    
    for i, img in enumerate(imgs):
        grid.paste(img, box=(i%cols*w, i//cols*h))
    return grid


# def load_concept_image(images_path):
#     while not os.path.exists(str(images_path)):
#         print('The images_path specified does not exist, use the colab file explorer to copy the path :')
#         images_path=input("")
#     save_path = images_path

#     images = []
#     for file_path in os.listdir(save_path):
#         try:
#             image_path = os.path.join(save_path, file_path)
#             images.append(Image.open(image_path).resize((512, 512)))
#         except:
#             print(f"{image_path} is not a valid image, please make sure to remove this file from the directory otherwise the training could fail.")
#     return save_path
    

# create dataset 
imagenet_templates_small = [
    "a photo of a {}",
    "a rendering of a {}",
    "a cropped photo of the {}",
    "the photo of a {}",
    "a photo of a clean {}",
    "a photo of a dirty {}",
    "a dark photo of the {}",
    "a photo of my {}",
    "a photo of the cool {}",
    "a close-up photo of a {}",
    "a bright photo of the {}",
    "a cropped photo of a {}",
    "a photo of the {}",
    "a good photo of the {}",
    "a photo of one {}",
    "a close-up photo of the {}",
    "a rendition of the {}",
    "a photo of the clean {}",
    "a rendition of a {}",
    "a photo of a nice {}",
    "a good photo of a {}",
    "a photo of the nice {}",
    "a photo of the small {}",
    "a photo of the weird {}",
    "a photo of the large {}",
    "a photo of a cool {}",
    "a photo of a small {}",
]
# imagenet_templates_small = [
#     "a photo of {} on the brown sofa"
# ]

imagenet_style_templates_small = [
    "a painting in the style of {}",
    "a rendering in the style of {}",
    "a cropped painting in the style of {}",
    "the painting in the style of {}",
    "a clean painting in the style of {}",
    "a dirty painting in the style of {}",
    "a dark painting in the style of {}",
    "a picture in the style of {}",
    "a cool painting in the style of {}",
    "a close-up painting in the style of {}",
    "a bright painting in the style of {}",
    "a cropped painting in the style of {}",
    "a good painting in the style of {}",
    "a close-up painting in the style of {}",
    "a rendition in the style of {}",
    "a nice painting in the style of {}",
    "a small painting in the style of {}",
    "a weird painting in the style of {}",
    "a large painting in the style of {}",
]

# set up the dataset
class TextualInversionDataset(Dataset):
    def __init__(
        self,
        data_root,
        tokenizer,
        num_placeholder_tokens,
        learnable_property="object",  # [object, style]
        size=512,
        repeats=100,
        interpolation="bicubic",
        flip_p=0.5,
        set="train",
        center_crop=False,
    ):
        self.data_root = data_root
        self.tokenizer = tokenizer
        self.learnable_property = learnable_property
        self.size = size
        self.num_placeholder_tokens = num_placeholder_tokens
        self.center_crop = center_crop
        self.flip_p = flip_p
        
        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]

        self.num_images = len(self.image_paths)
        self._length = self.num_images

        if set == "train":
            self._length = self.num_images * repeats

        self.interpolation = {
            "linear": PIL.Image.LINEAR,
            "bilinear": PIL.Image.BILINEAR,
            "bicubic": PIL.Image.BICUBIC,
            "lanczos": PIL.Image.LANCZOS,
        }[interpolation]

        self.templates = imagenet_style_templates_small if learnable_property == "style" else imagenet_templates_small
        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)

    def __len__(self):
        return self._length

    def __getitem__(self, i):
        example = {}
        
        # Init Text prompt
        placeholder_string = generate_placeholder_token_string(self.num_placeholder_tokens)
        text = random.choice(self.templates).format(placeholder_string)
        example["input_ids"] = self.tokenizer(
            text,
            padding="max_length",
            truncation=True,
            max_length=self.tokenizer.model_max_length,
            return_tensors="pt",
        ).input_ids[0]

        
        
        image = Image.open(self.image_paths[i % self.num_images])
        if not image.mode == "RGB":
            image = image.convert("RGB")
        
        

        example["input_ids"] = self.tokenizer(
            text,
            padding="max_length",
            truncation=True,
            max_length=self.tokenizer.model_max_length,
            return_tensors="pt",
        ).input_ids[0]

        # default to score-sde preprocessing
        img = np.array(image).astype(np.uint8)

        if self.center_crop:
            crop = min(img.shape[0], img.shape[1])
            h, w, = (
                img.shape[0],
                img.shape[1],
            )
            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]

        image = Image.fromarray(img)
        image = image.resize((self.size, self.size), resample=self.interpolation)

        image = self.flip_transform(image)
        image = np.array(image).astype(np.uint8)
        image = (image / 127.5 - 1.0).astype(np.float32)

        example["pixel_values"] = torch.from_numpy(image).permute(2, 0, 1)
        
        return example
    
    
# set up the model
def init_models(model_path):
    tokenizer = CLIPTokenizer.from_pretrained('stabilityai/stable-diffusion-2', subfolder="tokenizer")
    tokenizer.save_pretrained('sd-concept-output/tokenizer')
    # load stable diffusion model
    text_encoder = CLIPTextModel.from_pretrained(
        model_path, subfolder="text_encoder"
    )
    vae = AutoencoderKL.from_pretrained(
        model_path, subfolder="vae"
    )
    unet = UNet2DConditionModel.from_pretrained(
        model_path, subfolder="unet"
    )
    noise_scheduler = DDPMScheduler.from_config(model_path, subfolder="scheduler")
    return tokenizer, text_encoder, vae, unet, noise_scheduler
    
# only need to optimize the embeddings
def freeze_params(params):
    for param in params:
        param.requires_grad = False

def freeze_model(vae, unet, text_encoder):
    # Freeze vae and unet
    freeze_params(vae.parameters())
    freeze_params(unet.parameters())
    # Freeze all parameters except for the token embeddings in text encoder
    params_to_freeze = itertools.chain(
        text_encoder.text_model.encoder.parameters(),
        text_encoder.text_model.final_layer_norm.parameters(),
        text_encoder.text_model.embeddings.position_embedding.parameters(),
    )
    freeze_params(params_to_freeze)
    return vae, unet, text_encoder
    

# def create_dataset(image_folder, what_to_teach, train_batch_size=1):
#     # create training data
#     train_dataset = TextualInversionDataset(
#         data_root=image_folder,
#         tokenizer=tokenizer,
#         size=512,
#         placeholder_token=placeholder_token,
    
#         repeats=100,
#         learnable_property=what_to_teach, #Option selected above between object and style
#         center_crop=False,
#         set="train"
#     )
#     train_dataloader = torch.utils.data.DataLoader(
#         train_dataset, 
#         batch_size=train_batch_size, 
#         shuffle=True
#     )
#     return train_dataset, train_dataloader
    


def save_progress(text_encoder, placeholder_tokens_to_ids, accelerator, save_path, logger):
    logger.info("Saving embeddings")
    learned_embeds_dict = {}
    for placeholder_token in placeholder_tokens_to_ids:
        placeholder_token_id = placeholder_tokens_to_ids[placeholder_token]
        learned_embeds = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[placeholder_token_id]
        learned_embeds_dict[placeholder_token] = learned_embeds.detach().cpu()
    torch.save(learned_embeds_dict, save_path)

def training_function(
    text_encoder, 
    vae, 
    unet, 
    placeholder_tokens_to_ids,
    pretrained_model_name_or_path, 
    train_dataset, 
    train_dataloader, 
    noise_scheduler, 
    logger
):
    train_batch_size = hyperparameters["train_batch_size"]
    gradient_accumulation_steps = hyperparameters["gradient_accumulation_steps"]
    learning_rate = hyperparameters["learning_rate"]
    max_train_steps = hyperparameters["max_train_steps"]
    output_dir = hyperparameters["output_dir"]
    gradient_checkpointing = hyperparameters["gradient_checkpointing"]

    accelerator = Accelerator(
        gradient_accumulation_steps=gradient_accumulation_steps,
        mixed_precision=hyperparameters["mixed_precision"]
    )

    if gradient_checkpointing:
        text_encoder.gradient_checkpointing_enable()
        unet.enable_gradient_checkpointing()

    # train_dataloader = create_dataloader(train_batch_size)

    if hyperparameters["scale_lr"]:
        learning_rate = (
            learning_rate * gradient_accumulation_steps * train_batch_size * accelerator.num_processes
        )

    # Initialize the optimizer
    optimizer = torch.optim.AdamW(
        text_encoder.get_input_embeddings().parameters(),  # only optimize the embeddings
        lr=learning_rate,
    )

    text_encoder, optimizer, train_dataloader = accelerator.prepare(
        text_encoder, optimizer, train_dataloader
    )

    weight_dtype = torch.float32
    if accelerator.mixed_precision == "fp16":
        weight_dtype = torch.float16
    elif accelerator.mixed_precision == "bf16":
        weight_dtype = torch.bfloat16

    # Move vae and unet to device
    vae.to(accelerator.device, dtype=weight_dtype)
    unet.to(accelerator.device, dtype=weight_dtype)

    # Keep vae in eval mode as we don't train it
    vae.eval()
    # Keep unet in train mode to enable gradient checkpointing
    unet.train()

    # We need to recalculate our total training steps as the size of the training dataloader may have changed.
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)
    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)
    # Train!
    total_batch_size = train_batch_size * accelerator.num_processes * gradient_accumulation_steps

    logger.info("***** Running training *****")
    logger.info(f"  Num examples = {len(train_dataset)}")
    logger.info(f"  Instantaneous batch size per device = {train_batch_size}")
    logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
    logger.info(f"  Gradient Accumulation steps = {gradient_accumulation_steps}")
    logger.info(f"  Total optimization steps = {max_train_steps}")
    # Only show the progress bar once on each machine.
    progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)
    progress_bar.set_description("Steps")
    global_step = 0
    
    loss_arr = []

    for epoch in range(num_train_epochs):
        text_encoder.train()
        for step, batch in enumerate(train_dataloader):
            with accelerator.accumulate(text_encoder):
                # Convert images to latent space
                # breakpoint()
                latents = vae.encode(batch["pixel_values"].to(dtype=weight_dtype)).latent_dist.sample().detach()
                latents = latents * 0.18215

                # Sample noise that we'll add to the latents
                noise = torch.randn_like(latents)
                bsz = latents.shape[0]
                # Sample a random timestep for each image
                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device).long()

                # Add noise to the latents according to the noise magnitude at each timestep
                # (this is the forward diffusion process)
                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

                # Get the text embedding for conditioning
                encoder_hidden_states = text_encoder(batch["input_ids"])[0]

                # Predict the noise residual
                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states.to(weight_dtype)).sample

                 # Get the target for loss depending on the prediction type
                if noise_scheduler.config.prediction_type == "epsilon":
                    target = noise
                elif noise_scheduler.config.prediction_type == "v_prediction":
                    target = noise_scheduler.get_velocity(latents, noise, timesteps)
                else:
                    raise ValueError(f"Unknown prediction type {noise_scheduler.config.prediction_type}")
                

                loss = F.mse_loss(noise_pred, target, reduction="none").mean([1, 2, 3]).mean()
                loss_arr.append(loss.item())
                
                # writer.add_scalar('MSE', loss.item(), int(global_step))
             
                accelerator.backward(loss)

                # Zero out the gradients for all token embeddings except the newly added
                # embeddings for the concept, as we only want to optimize the concept embeddings
                if accelerator.num_processes > 1:
                    grads = text_encoder.module.get_input_embeddings().weight.grad
                else:
                    grads = text_encoder.get_input_embeddings().weight.grad
                # Get the index for tokens that we want to zero the grads for
                index_grads_to_zero = torch.arange(len(tokenizer)) <= 49407 # TODO: brute force, receive argument is better
                grads.data[index_grads_to_zero, :] = grads.data[index_grads_to_zero, :].fill_(0)
                
                # if global_step == 0:
                #     save_path = os.path.join(output_dir, f"learned_embeds-step-{global_step}.bin")
                #     save_progress(text_encoder, placeholder_tokens_to_ids, accelerator, save_path, logger)

                optimizer.step()
                optimizer.zero_grad()

            # Checks if the accelerator has performed an optimization step behind the scenes
            if accelerator.sync_gradients:
                progress_bar.update(1)
                
                if global_step in hyperparameters["save_steps"]:
                    save_path = os.path.join(output_dir, f"learned_embeds-step-{global_step}.bin")
                    save_progress(text_encoder, placeholder_tokens_to_ids, accelerator, save_path, logger)
                    
                global_step += 1
                # if global_step % hyperparameters["vis_dist"] == 0:
                #   with torch.no_grad():
                #     cur_embed = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[placeholder_token_id].cpu().clone()
                #     euclidean_distance = torch.norm(init_embedding - cur_embed)
                #     writer.add_scalar('Distance2Initial', euclidean_distance, global_step)


            logs = {"loss": loss.detach().item()}
            progress_bar.set_postfix(**logs)

            if global_step >= max_train_steps:
                break

        accelerator.wait_for_everyone()

    
    # Create the pipeline using using the trained modules and save it.
    if accelerator.is_main_process:
        pipeline = StableDiffusionPipeline.from_pretrained(
            pretrained_model_name_or_path,
            text_encoder=accelerator.unwrap_model(text_encoder),
            tokenizer=tokenizer,
            vae=vae,
            unet=unet,
        )
        #pipeline.save_pretrained(output_dir)
        
        # Also save the newly trained embeddings
        save_path = os.path.join(output_dir, f"learned_embeds.bin")
        save_progress(text_encoder, placeholder_tokens_to_ids, accelerator, save_path, logger)
        losses = np.array(loss_arr)
        np.save(f'{output_dir}/loss.npy',losses)
  
def generate_placeholder_token_string(num_placeholder):
    """
    ...
    """
    placeholder_token_lst = []
    for i in range(num_placeholder):  
        placeholder_token_lst.append(f"\u003C{i}>")
    
    placeholder_string = " ".join(placeholder_token_lst)
    return placeholder_string


        
def init_mutli_tokens(caption, text_encoder, tokenizer) -> Dict[str, int]:
    """
    Initialize multiple new tokens for the input text_encoder, with the tokens 
    in caption.
    
    Args: 
        caption (str): either class labels or longer/more descriptive strings
        text_encoder: from stable diffusion
        tokenizer: from stable diffusion text encoder
    
    ReturnL
        placeholder_tokens_to_ids (dict): Maps defined placeholder token to its
            id in the tokenizer
    """
    
    captions_tokens = tokenizer.tokenize(caption)
    placeholder_tokens_to_ids = {}
    
    for i in range(len(captions_tokens)):
        placeholder_token = f"\u003C{i}>"
        tokenizer.add_tokens(placeholder_token)
        
        init_token_id = tokenizer.convert_tokens_to_ids(captions_tokens[i])

        placeholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)
        
        placeholder_tokens_to_ids[placeholder_token] =  placeholder_token_id
        
        print(f"Special token '{placeholder_token}' is added to tokenizer, \
                it has id {placeholder_token_id}")
        text_encoder.resize_token_embeddings(len(tokenizer))
        token_embeds = text_encoder.get_input_embeddings().weight.data

        token_embeds[placeholder_token_id] = token_embeds[init_token_id]

    return placeholder_tokens_to_ids




if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--target_image_name', type=str, required=True)
    parser.add_argument('--initialization', type=str, required=True)
    parser.add_argument('--init_type', type=str, required=True)  # one of {null, class, caption}
    
    args = parser.parse_args()
    
    target_image_name = args.target_image_name 
    init_string = args.initialization
    init_type = args.init_type

    logger = get_logger(__name__)
    
    pretrained_model_name_or_path = "sd-concept-output"
    what_to_teach = "object" 
    
   
    concept_images_path = "textual_inversion/images/{}_small".format(target_image_name)
    # save_path = load_concept_image(concept_images_path)
    output_path = f"{target_image_name}/{init_type}_{init_string}_init"
    writer = SummaryWriter(f'/root/tf-logs/{output_path}')
    os.makedirs(output_path, exist_ok=True)
    save_steps = [i * 5 for i in range(20)] + [i * 20 + 100 for i in range(0, 46)] + [i * 250 + 1000 for i in range(0, 17)]
    hyperparameters = {
        "learning_rate": 5e-04,
        "scale_lr": True,
        "max_train_steps": 5001,
        "save_steps": save_steps,
        "train_batch_size": 4,
        "gradient_accumulation_steps": 1,
        "gradient_checkpointing": True,
        "mixed_precision": "fp16",
        "seed": 3407,
        "output_dir": output_path,
        "vis_dist": 50
    }
    
    tokenizer, text_encoder, vae, unet, noise_scheduler = init_models(pretrained_model_name_or_path)
    
    
    
    ################################
    init_string_tokens = tokenizer.tokenize(init_string)
    num_placeholder_tokens = len(init_string_tokens)
    
    placeholder_tokens_to_ids = init_mutli_tokens(caption=init_string, 
                                                  text_encoder=text_encoder, 
                                                  tokenizer=tokenizer)
    
    
    
    ################################
#     # add the placeholder token
#     num_added_tokens = tokenizer.add_tokens(placeholder_token)

#     if num_added_tokens == 0:
#         raise ValueError(
#             f"The tokenizer already contains the token {placeholder_token}. Please pass a different"
#             " `placeholder_token` that is not already in the tokenizer."
#         )
#     token_ids = tokenizer.encode(initializer_token, add_special_tokens=False)
#     # Check if initializer_token is a single token or a sequence of tokens
#     if len(token_ids) > 1:
#         raise ValueError("The initializer token must be a single token.")
    
#     placeholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)
    
#     text_encoder.resize_token_embeddings(len(tokenizer))
#     token_embeds = text_encoder.get_input_embeddings().weight.data
    
#     token_embeds[placeholder_token_id] = token_embeds[token_ids]
    
    
    # freeze parameters
    vae, net, text_encoder = freeze_model(vae, unet, text_encoder)


    train_dataset = TextualInversionDataset(
        data_root=concept_images_path, 
        tokenizer=tokenizer,
        size=512,
        num_placeholder_tokens=num_placeholder_tokens,
        repeats=100,
        learnable_property=what_to_teach, #Option selected above between object and style
        center_crop=False,
        set="train"
    )
    train_dataloader = torch.utils.data.DataLoader(
        train_dataset, 
        batch_size=hyperparameters['train_batch_size'], 
        shuffle=True
    )

   
    # begin training
    accelerate.notebook_launcher(
        training_function, 
        args=(text_encoder, 
              vae, 
              unet, 
              placeholder_tokens_to_ids,
              pretrained_model_name_or_path, 
              train_dataset, 
              train_dataloader, 
              noise_scheduler,
              logger), 
        num_processes=1
    )
    
    for param in itertools.chain(unet.parameters(), text_encoder.parameters()):
        if param.grad is not None:
            del param.grad  # free some memory
        torch.cuda.empty_cache()

# sanity check
# torch.all(text_encoder.get_input_embeddings().weight.data == text_encoder.get_input_embeddings().weight.data[-1], dim=1).nonzero()

# tokenizer.decoder('*')